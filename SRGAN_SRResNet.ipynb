{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981c854c",
      "metadata": {
        "id": "981c854c",
        "outputId": "554f8821-970f-48d7-f050-cd29cabb1ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.4.3\n",
            "  Downloading tensorflow-2.4.3-cp38-cp38-manylinux2010_x86_64.whl (394.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.6/394.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (3.3.0)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (3.19.6)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 KB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (1.12)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (0.38.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (1.6.3)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.4.3) (2.9.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow==2.4.3) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.3) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.3) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.3) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.3) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.3) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.3) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.3) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.3) (3.2.2)\n",
            "Building wheels for collected packages: termcolor, wrapt\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=423a00b13f4341578f7c35642307e127f9cfe0324d465b72fd6cfe4a3ab15f6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=72347 sha256=337baf9b222e1ef2247d231936f44c9ee61f72aae6827e94f74c6ecbb8b13157\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
            "Successfully built termcolor wrapt\n",
            "Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, numpy, grpcio, gast, absl-py, h5py, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.2.0\n",
            "    Uninstalling termcolor-2.2.0:\n",
            "      Successfully uninstalled termcolor-2.2.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.51.1\n",
            "    Uninstalling grpcio-1.51.1:\n",
            "      Successfully uninstalled grpcio-1.51.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.3.0\n",
            "    Uninstalling absl-py-1.3.0:\n",
            "      Successfully uninstalled absl-py-1.3.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "xarray-einstats 0.4.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "pydantic 1.10.4 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.32.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.1 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.32.0 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-0.15.0 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.3 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n",
            "Cloning into 'image-super-resolution'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Total 150 (delta 0), reused 0 (delta 0), pack-reused 150\u001b[K\n",
            "Receiving objects: 100% (150/150), 36.74 MiB | 14.73 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ]
        }
      ],
      "source": [
        "# uncomment and run the lines below if running in google colab\n",
        "!pip install tensorflow==2.4.3\n",
        "!git clone https://github.com/jlaihong/image-super-resolution.git\n",
        "!mv image-super-resolution/* ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T6_iSI3gOOd",
        "outputId": "45d0b7b7-e804-4567-dc97-91d19c170497"
      },
      "id": "-T6_iSI3gOOd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e7277a",
      "metadata": {
        "id": "a3e7277a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import tensorflow as tf\n",
        "\n",
        "from datasets.div2k.parameters import Div2kParameters \n",
        "from models.srresnet import build_srresnet\n",
        "from models.pretrained import pretrained_models\n",
        "from utils.prediction import get_sr_image\n",
        "from utils.config import config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe72874",
      "metadata": {
        "id": "2fe72874"
      },
      "outputs": [],
      "source": [
        "dataset_key = \"bicubic_x4\"\n",
        "\n",
        "data_path = config.get(\"data_path\", \"\") \n",
        "\n",
        "div2k_folder = os.path.abspath(os.path.join(data_path, \"div2k\"))\n",
        "\n",
        "dataset_parameters = Div2kParameters(dataset_key, save_data_directory=div2k_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9b6c5b",
      "metadata": {
        "id": "5a9b6c5b"
      },
      "outputs": [],
      "source": [
        "def load_image(path):\n",
        "    img = Image.open(path)\n",
        "    \n",
        "    was_grayscale = len(img.getbands()) == 1\n",
        "    \n",
        "    if was_grayscale or len(img.getbands()) == 4:\n",
        "        img = img.convert('RGB')\n",
        "    \n",
        "    return was_grayscale, np.array(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ddafc3",
      "metadata": {
        "id": "53ddafc3"
      },
      "outputs": [],
      "source": [
        "#model_name = \"srresnet\"\n",
        "model_name = \"srgan\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0187e4",
      "metadata": {
        "id": "ee0187e4"
      },
      "outputs": [],
      "source": [
        "model_key = f\"{model_name}_{dataset_key}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6dd2475",
      "metadata": {
        "id": "d6dd2475",
        "outputId": "961108de-d683-43e3-c0d4-92b86546bb9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couldn't find file:  /content/weights/srgan_bicubic_x4/generator.h5 , attempting to download a pretrained model\n",
            "Downloading data from https://image-super-resolution-weights.s3.af-south-1.amazonaws.com/srgan_bicubic_x4/generator.h5\n",
            "6529024/6521184 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "weights_directory = os.path.abspath(f\"weights/{model_key}\")\n",
        "\n",
        "file_path = os.path.join(weights_directory, \"generator.h5\")\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    os.makedirs(weights_directory, exist_ok=True)\n",
        "    \n",
        "    print(\"Couldn't find file: \", file_path, \", attempting to download a pretrained model\")\n",
        "    \n",
        "    if model_key not in pretrained_models:\n",
        "        print(f\"Couldn't find pretrained model with key: {model_key}, available pretrained models: {pretrained_models.key()}\")\n",
        "    else:\n",
        "        download_url = pretrained_models[model_key]\n",
        "        file = file_path.split(\"/\")[-1]\n",
        "        tf.keras.utils.get_file(file, download_url, cache_subdir=weights_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89375f5a",
      "metadata": {
        "id": "89375f5a"
      },
      "outputs": [],
      "source": [
        "model = build_srresnet(scale=dataset_parameters.scale)\n",
        "\n",
        "os.makedirs(weights_directory, exist_ok=True)\n",
        "weights_file = f'{weights_directory}/generator.h5'\n",
        "\n",
        "model.load_weights(weights_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/Datasets/test_report/baboon_LR.png\"\n",
        "was_grayscale, lr = load_image(image_path)\n",
        "\n",
        "sr = get_sr_image(model, lr)\n",
        "    \n",
        "if was_grayscale:\n",
        "    sr = ImageOps.grayscale(sr)\n",
        "\n",
        "image_name = image_path.split(\"/\")[-1]\n",
        "sr.save(\"/content/drive/MyDrive/Datasets/test_report/baboon_SRGAN.png\" )"
      ],
      "metadata": {
        "id": "Q0AvlKzH91E6"
      },
      "id": "Q0AvlKzH91E6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/Datasets/test_report/cartoon_LR.png\"\n",
        "was_grayscale, lr = load_image(image_path)\n",
        "\n",
        "sr = get_sr_image(model, lr)\n",
        "    \n",
        "if was_grayscale:\n",
        "    sr = ImageOps.grayscale(sr)\n",
        "\n",
        "image_name = image_path.split(\"/\")[-1]\n",
        "sr.save(\"/content/drive/MyDrive/Datasets/test_report/cartoon_SRGAN.png\" )"
      ],
      "metadata": {
        "id": "XWulIZ6wpIZT"
      },
      "id": "XWulIZ6wpIZT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.mkdir(\"/content/drive/MyDrive/Datasets/output\")"
      ],
      "metadata": {
        "id": "ca6ryecuzT8L"
      },
      "id": "ca6ryecuzT8L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr_img_path = \"/content/drive/MyDrive/Datasets/test_LR/X4\"\n",
        "# os.mkdir(\"/content/drive/MyDrive/Datasets/output/SRGAN_X4\")\n",
        "# for i in range(len(os.listdir(lr_img_path))):\n",
        "#   image = os.listdir(lr_img_path)[i]\n",
        "#   if i%100 == 0 : print(i)\n",
        "\n",
        "#   image_paths = glob.glob(lr_img_path + \"/\" + image )\n",
        "\n",
        "#   for image_path in image_paths:\n",
        "#       #print(image_path)\n",
        "#       was_grayscale, lr = load_image(image_path)\n",
        "      \n",
        "#       sr = get_sr_image(model, lr)\n",
        "          \n",
        "#       if was_grayscale:\n",
        "#           sr = ImageOps.grayscale(sr)\n",
        "      \n",
        "#       image_name = image_path.split(\"/\")[-1]\n",
        "#       sr.save(\"/content/drive/MyDrive/Datasets/output/SRGAN_X4/\"+image_name )"
      ],
      "metadata": {
        "id": "xI74bnuA3zGo"
      },
      "id": "xI74bnuA3zGo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr_img_path = \"/content/drive/MyDrive/Datasets/test_LR/X3\"\n",
        "# os.mkdir(\"/content/drive/MyDrive/Datasets/output/SRGAN_X3\")\n",
        "# for i in range(len(os.listdir(lr_img_path))):\n",
        "#   image = os.listdir(lr_img_path)[i]\n",
        "#   if i%100 == 0 : print(i)\n",
        "\n",
        "#   image_paths = glob.glob(lr_img_path + \"/\" + image )\n",
        "\n",
        "#   for image_path in image_paths:\n",
        "#       #print(image_path)\n",
        "#       was_grayscale, lr = load_image(image_path)\n",
        "      \n",
        "#       sr = get_sr_image(model, lr)\n",
        "          \n",
        "#       if was_grayscale:\n",
        "#           sr = ImageOps.grayscale(sr)\n",
        "      \n",
        "#       image_name = image_path.split(\"/\")[-1]\n",
        "#       sr.save(\"/content/drive/MyDrive/Datasets/output/SRGAN_X3/\"+image_name )"
      ],
      "metadata": {
        "id": "13Dg8C8CO9Q4"
      },
      "id": "13Dg8C8CO9Q4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr_img_path = \"/content/drive/MyDrive/Datasets/test_LR/X2\"\n",
        "# os.mkdir(\"/content/drive/MyDrive/Datasets/output/SRGAN_X2\")\n",
        "# for i in range(len(os.listdir(lr_img_path))):\n",
        "#   image = os.listdir(lr_img_path)[i]\n",
        "#   if i%100 == 0 : print(i)\n",
        "\n",
        "#   image_paths = glob.glob(lr_img_path + \"/\" + image )\n",
        "\n",
        "#   for image_path in image_paths:\n",
        "#       #print(image_path)\n",
        "#       was_grayscale, lr = load_image(image_path)\n",
        "      \n",
        "#       sr = get_sr_image(model, lr)\n",
        "          \n",
        "#       if was_grayscale:\n",
        "#           sr = ImageOps.grayscale(sr)\n",
        "      \n",
        "#       image_name = image_path.split(\"/\")[-1]\n",
        "#       sr.save(\"/content/drive/MyDrive/Datasets/output/SRGAN_X2/\"+image_name )"
      ],
      "metadata": {
        "id": "MdagLzZHO9TL"
      },
      "id": "MdagLzZHO9TL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}